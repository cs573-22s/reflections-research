The paper "Investigating the Effect of the Multiple Comparisons Problem in Visual Analysis" by Zgraggen et al. addresses the multiple comparisons problem (MCP) in visualization. Patterns in visualizations may simply be noise rather than actual trends in data. The probability of false positives increases as more visualizations are examined and the number of comparisons made increases. Zgraggen et al. observe that the multiple comparisons problem is often overlooked in visual analysis and define a method "to evaluate MCP in visualization tools by measuring the accuracy of user reported insights on synthetic datasets with known ground truth labels". Using this experiment, they found that more than 60% of user insights were false. They implement a confirmatory analysis approach accounting for all visual comparisons and show that this approach achieves similar results to one requiring a validation dataset. 

This paper interests me as I had not previously considered the multiple comparisons problem as applied to visualizations. Zgraggen et al. observe that visualization tools are often marketed as being able to quickly and effortlessly provide hidden data insights without statistical expertise, falsely leading users to believe that any insights found through exploratory visual analysis are highly reliable. This is an important observation given how widespread visualization tools like Tableau are. In future projects involving data collection, I will be mindful of how MCP may affect the accuracy of the conclusions I draw from exploratory data visualizations. 

Link: http://emanuelzgraggen.com/assets/pdf/risk.pdf
